{
  "comments": [
    {
      "key": {
        "uuid": "9a5c5d37_7309de2b",
        "filename": "ci/merge.sh",
        "patchSetId": 2
      },
      "lineNbr": 16,
      "author": {
        "id": 2034
      },
      "writtenOn": "2015-11-24T04:19:54Z",
      "side": 1,
      "message": "If the python-nose-cov package is not installed, nothing will happen here.  Still need to update the JJB to report code coverage reports at some point.",
      "revId": "002920e29d7fa4a28abec96773b470c90bafe55d",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "9a5c5d37_3313665e",
        "filename": "ci/verify.sh",
        "patchSetId": 2
      },
      "lineNbr": 13,
      "author": {
        "id": 2034
      },
      "writtenOn": "2015-11-24T04:19:54Z",
      "side": 1,
      "message": "This way if flake8 is not installed on the build server, no harm done, we will pass this step.",
      "range": {
        "startLine": 13,
        "startChar": 16,
        "endLine": 13,
        "endChar": 17
      },
      "revId": "002920e29d7fa4a28abec96773b470c90bafe55d",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "9a5c5d37_53037a73",
        "filename": "storperf/test_executor.py",
        "patchSetId": 2
      },
      "lineNbr": 58,
      "author": {
        "id": 830
      },
      "writtenOn": "2015-11-24T17:02:46Z",
      "side": 1,
      "message": "Minor nit for clarity: it looks like the message content could be made a little more clear. E.g.: \"Avg Latency (us) Read/Write: \" + read_latency + \"/\" + write_latency + \" IOPS Read/Write: \" + read_iops + \"/\" + write_iops\n\nCan we make the value captured in ms instead of us? The lowest (non-RAM disk) value I would expect to see is in the hundreds of us, whereas the highest value will be in the 10\u0027s of ms.",
      "revId": "002920e29d7fa4a28abec96773b470c90bafe55d",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "9a5c5d37_333fe627",
        "filename": "storperf/test_executor.py",
        "patchSetId": 2
      },
      "lineNbr": 58,
      "author": {
        "id": 2034
      },
      "writtenOn": "2015-11-24T20:48:37Z",
      "side": 1,
      "message": "Yes, I need to clean this up a little.",
      "parentUuid": "9a5c5d37_53037a73",
      "revId": "002920e29d7fa4a28abec96773b470c90bafe55d",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "9a5c5d37_33f8067c",
        "filename": "storperf/test_executor.py",
        "patchSetId": 2
      },
      "lineNbr": 72,
      "author": {
        "id": 830
      },
      "writtenOn": "2015-11-24T17:02:46Z",
      "side": 1,
      "message": "Just verifying behavior: if no workloads are specified in command line, then you will run all of the workloads in the workload_dir? If so, nice. I like the ease in adding new workloads.",
      "revId": "002920e29d7fa4a28abec96773b470c90bafe55d",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "9a5c5d37_533a5a37",
        "filename": "storperf/test_executor.py",
        "patchSetId": 2
      },
      "lineNbr": 72,
      "author": {
        "id": 2034
      },
      "writtenOn": "2015-11-24T20:48:37Z",
      "side": 1,
      "message": "Correct - this is the implementation of default being \u0027all\u0027.",
      "parentUuid": "9a5c5d37_33f8067c",
      "revId": "002920e29d7fa4a28abec96773b470c90bafe55d",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "9a5c5d37_13f50283",
        "filename": "storperf/test_executor.py",
        "patchSetId": 2
      },
      "lineNbr": 126,
      "author": {
        "id": 830
      },
      "writtenOn": "2015-11-24T17:02:46Z",
      "side": 1,
      "message": "We may need to reverse the order of these for loops. I think you want to expand the number of streams of the workload before changing to the next workload. Ordinarily, it would not matter, but for SSD, I understand major context switching of workloads does impact its behavior. See SNIA Performance WP. This can be left as TBD for now.",
      "revId": "002920e29d7fa4a28abec96773b470c90bafe55d",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "9a5c5d37_133ce22e",
        "filename": "storperf/test_executor.py",
        "patchSetId": 2
      },
      "lineNbr": 126,
      "author": {
        "id": 2034
      },
      "writtenOn": "2015-11-24T20:48:37Z",
      "side": 1,
      "message": "Fair enough.  The question I also have is: do we redo the SSD preconditioning before the next workload?",
      "parentUuid": "9a5c5d37_13f50283",
      "revId": "002920e29d7fa4a28abec96773b470c90bafe55d",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    }
  ]
}